{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"fake_news_detection.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ic57cdwLZ-L1"},"source":["#Loading our datasets"]},{"cell_type":"code","metadata":{"id":"fxa_6uImXB8W"},"source":["from google.colab import drive\n","from os import chdir\n","\n","drive.mount(\"/content/drive\")\n","chdir(\"/content/drive/MyDrive/my_project1\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ORwyt-ypaGN7"},"source":["#Imports\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import re"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ngm1NIMSaF_X"},"source":["legit_news = pd.read_csv(\"True.csv\")\n","fake_news = pd.read_csv(\"Fake.csv\")\n","\n","# keep a clean dataset for bonus part\n","legit_news_unproc = legit_news.copy()\n","fake_news_unproc = fake_news.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9aLt0iodQg7v"},"source":["#Getting familiar with the data"]},{"cell_type":"code","metadata":{"id":"_UMVdi03aglp"},"source":["legit_news.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gEVKTvFUapq-"},"source":["fake_news.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EfYPBBvSetoJ"},"source":["print(legit_news.subject.unique().tolist())\n","print(fake_news.subject.unique().tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7JAfw_sazef"},"source":["print(legit_news.info(),'\\n\\n')\n","print(fake_news.info())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v-f3L_-hbbio"},"source":["##From the above information we can tell that we have no null values in our datasets"]},{"cell_type":"markdown","metadata":{"id":"XfJz4zZ9kvVr"},"source":["#1) Προεπεξεργασία/καθάρισμα"]},{"cell_type":"code","metadata":{"id":"uUp_FCDjgc6Z"},"source":["import nltk\n","from nltk.corpus.reader.wordnet import NOUN\n","from nltk.corpus import wordnet\n","\n","# Importing the WordNetLemmatizer module from nltk.stem\n","nltk.download('wordnet')\n","\n","class WordNetLemmatizer(object):\n","  def __init__(self):\n","        pass\n","\n","  def lemmatize(self, word, pos=NOUN):\n","        lemmas = wordnet._morphy(word, pos)\n","        return min(lemmas, key=len) if lemmas else word\n","\n","\n","  def __repr__(self):\n","        return \"<WordNetLemmatizer>\"\n","\n","\n","\n","# unload wordnet\n","def teardown_module(module=None):\n","    from nltk.corpus import wordnet\n","\n","    wordnet._unload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DXHfnWsydI0h"},"source":["# Create WordNetLemmatizer object\n","lemmatizer = WordNetLemmatizer()\n","\n","# Text pre-processing function using regular expresions and lemmatizer\n","def tidyText(text):\n","    # cleaning the text from special characters i.e. punctuation\n","    text = re.sub(r'[^\\w\\- ]',' ',text)\n","\n","    # lemmatize the words\n","    tokens = text.split()\n","    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n","    # remove leftover characters\n","    lemmas = list(filter(lambda token: len(token) > 2, lemmas))\n","    text = \" \".join(lemmas)\n","    \n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ruXUt-_akSai"},"source":["title_idx = fake_news.columns.to_list().index('title')\n","text_idx = fake_news.columns.to_list().index('text')\n","\n","print(fake_news.iloc[5, title_idx])\n","print(fake_news.iloc[5, text_idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kzdFYs6PgkLn"},"source":["# Convert all text to lowercase and apply pre-processing function\n","for column in ['title', 'text']:\n","    fake_news[column] = fake_news[column].str.lower().apply(tidyText)\n","    legit_news[column] = legit_news[column].str.lower().apply(tidyText)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dr3OmvLLj70L"},"source":["print(fake_news.iloc[5, title_idx])\n","print(fake_news.iloc[5, text_idx])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OPqBUM7LwWlO"},"source":["def dropEmptyRows(df):\n","    empty_rows = df[(df['title']=='') | (df['text']=='')].index.to_list()\n","    df.drop(labels= empty_rows, inplace=True)\n","\n","dropEmptyRows(legit_news)\n","dropEmptyRows(fake_news)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tbvvnidTXkrg"},"source":["#2) Μελέτη των δεδομένων"]},{"cell_type":"markdown","metadata":{"id":"hyQp8nPyY74w"},"source":["##α."]},{"cell_type":"code","metadata":{"id":"CYnQyNxO583L"},"source":["from wordcloud import WordCloud, STOPWORDS\n","import requests\n","from PIL import Image, ImageOps\n","\n","def showWordClouds(mask_path, df):\n","    # get the image mask that will be used in the word cloud\n","    mask = Image.open(mask_path)\n","    mask = ImageOps.grayscale(mask)\n","    mask = np.array(mask)\n","\n","    stopwords = set(STOPWORDS).union({\"say\",\"may\",\"see\",\"will\"})\n","\n","    # concatenate all the titles from the corresponding column\n","    words = \" \".join(df['title'].to_list())\n","\n","    fig = plt.figure(figsize = (35, 25), facecolor = None)\n","    ax = fig.subplots()\n","\n","    wordcloud = WordCloud(background_color ='black',\n","                    colormap = 'autumn',\n","                    stopwords = stopwords,\n","                    min_font_size = 10,\n","                    mask= mask).generate(words)  \n","    \n","    ax.imshow(wordcloud)\n","    ax.axis(\"off\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jkWJ0YyJ7SUS"},"source":["### Non-fake news wordcloud"]},{"cell_type":"code","metadata":{"id":"INLKv3e_7HPz"},"source":["showWordClouds('legit.png', legit_news)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fZGH57807Y4M"},"source":["### Fake news wordcloud"]},{"cell_type":"code","metadata":{"id":"cyJJ7vZ07a4C"},"source":["showWordClouds('fake.png', fake_news)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vkjThBeFXtlA"},"source":["##β."]},{"cell_type":"code","metadata":{"id":"EmxPjRmlOzhs"},"source":["# list containing the means of the characters count of row 'title' and 'text'\n","mean_title = [legit_news['title'].str.len().mean(), fake_news['title'].str.len().mean()]\n","mean_text = [legit_news['text'].str.len().mean(), fake_news['text'].str.len().mean()]\n","\n","# plotting the results\n","fig, ax = plt.subplots(1,2, figsize = (12,6))\n","categories = ['Legit News', 'Fake News']\n","\n","ax[0].bar(categories, mean_title, color=(0.2, 0.4, 0.1, 0.6), edgecolor='black')\n","ax[0].grid(True)\n","ax[0].set_title('Title column character count mean')\n","ax[0].set_ylabel('Mean')\n","\n","ax[1].bar(categories, mean_text, color=(0.5, 0.2, 0.3, 0.6), edgecolor='black')\n","ax[1].grid(True)\n","ax[1].set_title('Text column character count mean')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vSmYTbT8wn50"},"source":["##γ."]},{"cell_type":"code","metadata":{"id":"VuQ_ztXIwrnO"},"source":["df_legit = legit_news.copy()\n","df_fake = fake_news.copy()\n","\n","def dropStopwords(df, column):\n","    stopwords = set(STOPWORDS).union({\"say\",\"may\",\"see\",\"will\"})\n","    # get the column as a series\n","    col = df[column]\n","    # split the text into list of words\n","    words = col.str.split()\n","    # iterate series and filter out stopwords\n","    filtered_words = words.apply(lambda tokens: list(filter(lambda token: token not in stopwords, tokens)))\n","    # join the words back into text\n","    df[column] = filtered_words.str.join(' ')\n","\n","def getWordCounts(df, rmvStopwrds=False):\n","    # remove the stopwords if needed\n","    if rmvStopwrds:\n","        print(f'Before stopword removal:\\n\\\n","\\tTitle length: {len(df[\"title\"].iloc[0].split())}\\n\\\n","\\tcontent: {df[\"title\"].iloc[0]}')\n","        dropStopwords(df, 'title')\n","        dropStopwords(df, 'text')\n","        print(f'After stopword removal:\\n\\\n","\\tTitle length: {len(df[\"title\"].iloc[0].split())}\\n\\\n","\\tcontent: {df[\"title\"].iloc[0]}')\n","    \n","    df['title_word_count'] = df['title'].str.split().str.len()\n","    df['text_word_count'] = df['text'].str.split().str.len()\n","\n","    fig = plt.figure(figsize=(18,10))\n","    ax = fig.subplots(1,2)\n","    sns.set_theme(context='talk',\n","                  font_scale=0.8,\n","                  palette='Oranges_r' if rmvStopwrds else 'coolwarm')\n","\n","    plt.sca(ax[0])\n","    sns.histplot(data=df,\n","                 x=\"title_word_count\",\n","                 bins=df['title_word_count'].unique().size,\n","                 edgecolor='black')\n","    ax[0].set_xlabel('Word count bins')\n","    ax[0].set_ylabel('Article count')\n","    ax[0].set_title('Title distribution plot')\n","\n","    plt.sca(ax[1])\n","    sns.histplot(data=df,\n","                 x=\"text_word_count\", \n","                 edgecolor='black',\n","                 bins=df['text_word_count'].unique().size//16)\n","    ax[1].set_title('Text distribution plot')\n","    ax[1].set_xlabel('Word count bins')\n","    ax[1].set_ylabel('Article count')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W5lmB4Ed_FGN"},"source":["###Non-fake news distributions"]},{"cell_type":"code","metadata":{"id":"lrQ0QIa0xzhb"},"source":["getWordCounts(df_legit)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_UVi74vQ_KBD"},"source":["###Fake news distributions"]},{"cell_type":"code","metadata":{"id":"zyB0yKDO_OTh"},"source":["getWordCounts(df_fake)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cvD3t_dwAxTD"},"source":["##δ."]},{"cell_type":"markdown","metadata":{"id":"v7ePwvddFQ4v"},"source":["###Non-fake news distributions [Stopwords Removed]\n"]},{"cell_type":"code","metadata":{"id":"AVbVIm-zAw75"},"source":["getWordCounts(df_legit, rmvStopwrds=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ymff49iiGD14"},"source":["###Fake news distributions [Stopwords Removed]\n"]},{"cell_type":"code","metadata":{"id":"h4Tg25AvGJGU"},"source":["getWordCounts(df_fake, rmvStopwrds=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gK_Y2qrjMpx"},"source":["##ε."]},{"cell_type":"code","metadata":{"id":"HTzO9F5SjLzg"},"source":["from nltk.util import ngrams\n","from itertools import islice\n","from collections import Counter\n","import itertools\n","\n","df_legit = legit_news.copy()\n","df_fake = fake_news.copy()\n","\n","def plotBigrams(df, column, N, axes):\n","    # filter out stopwords\n","    dropStopwords(df, column)\n","    \n","    col = df[column]\n","    # split sentences into list of words\n","    tokens = col.str.split()\n","    # iterate through series and get bigrams\n","    bigrams = tokens.apply(lambda tokens_row: list(ngrams(tokens_row, 2)))\n","    # merge bigrams into list\n","    bigrams = list(itertools.chain(*bigrams))\n","    counts = Counter(bigrams)\n","\n","    df = pd.DataFrame(data=counts.most_common(N), columns=['Bigrams','Count'])\n","    sns.barplot(data=df, \n","                y='Count',\n","                x='Bigrams',\n","                ax=axes[1],\n","                edgecolor= 'black')\n","    axes[1].set_title(column.capitalize() + ' column barplot', fontsize=18)\n","    axes[1].tick_params(axis='x', labelrotation=45)\n","    \n","    freqs = list(map(lambda item: (item[0][0]+' '+item[0][1], item[1]),\\\n","                     counts.most_common(N)))\n","    \n","    # print the wordcloud representation below the barchart\n","    wordcloud = WordCloud(width=1000, \n","                          height=600, \n","                          background_color ='black',\n","                          colormap = 'autumn',\n","                          min_font_size = 10).generate_from_frequencies(dict(freqs))\n","    axes[0].set_title(column.capitalize() + ' column wordcloud', fontsize=18)\n","    axes[0].imshow(wordcloud)\n","    axes[0].axis(\"off\")\n","\n","def topN_bigrams(df, N):\n","    fig = plt.figure(figsize=(28,18))\n","    # fig.tight_layout(pad=2.0)\n","    ax = fig.subplots(2,2)\n","    sns.set_theme(context='talk',\n","                  font_scale=0.8,\n","                  palette='deep')\n","\n","    plotBigrams(df, 'title', N, ax[:,0])\n","    plotBigrams(df, 'text', N, ax[:,1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7W3k6-7_Uxz6"},"source":["### Non-fake news"]},{"cell_type":"code","metadata":{"id":"IFL8_RBGU1v_"},"source":["topN_bigrams(df_legit, 20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDqodxPDU5-8"},"source":["### Fake news\n"]},{"cell_type":"code","metadata":{"id":"OfW6Df-sU5xF"},"source":["topN_bigrams(df_fake, 20)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UeXaY2Atd-E2"},"source":["#3) Δημιουργία συνόλου εκμάθησης και δοκιμής"]},{"cell_type":"code","metadata":{"id":"q-fBepOseODS"},"source":["df_legit = legit_news.copy()\n","df_fake = fake_news.copy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8pVZR-NQiOaq"},"source":["##Add the labels"]},{"cell_type":"code","metadata":{"id":"TEkGTD7QiTS-"},"source":["# make the new label columns\n","df_legit['label'] = 1\n","df_fake['label'] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bn4kaD5xhN3j"},"source":["##Separate both datasets into train and test datasets"]},{"cell_type":"code","metadata":{"id":"NToJgJJRZttl"},"source":["# function that splits dataset into two depending on percentage given as input\n","def splitTrainTest(df, train_percent):\n","    train_size = int(train_percent * df.shape[0])\n","\n","    train = df.iloc[:train_size, :]\n","    test = df.iloc[train_size:, :]\n","\n","    return train, test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mPDfdmWtgqNr"},"source":["lgt_train, lgt_test = splitTrainTest(df_legit, train_percent=0.9)\n","fk_train, fk_test = splitTrainTest(df_fake, train_percent=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pxjVBKncqEEQ"},"source":["def mergeDatasets(df_legit, df_fake, file_name):\n","    df_merged = pd.concat([df_legit, df_fake], ignore_index=False)\n","    df_merged.to_csv(file_name, index=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_otv6_K8hsr1"},"source":["mergeDatasets(lgt_train, fk_train, 'train.csv')\n","mergeDatasets(lgt_test, fk_test, 'test.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_coWH3zzicNE"},"source":["train = pd.read_csv('train.csv', index_col=0)\n","train.label.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Lkky5V6iizR"},"source":["test = pd.read_csv('test.csv', index_col=0)\n","test.label.value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MSogqeY4c5T-"},"source":["#4) Classification"]},{"cell_type":"markdown","metadata":{"id":"RjoSA_n1RguB"},"source":["## Below we filter out any records with invalid date formats in our datasets as we will use this column as a feature"]},{"cell_type":"code","metadata":{"id":"x7PZegutRfAM"},"source":["def findInvalidDates(date):\n","    isInvalid = False\n","    try:\n","        pd.to_datetime(date)\n","    except:\n","        isInvalid = True\n","\n","    return isInvalid\n","\n","\n","test.drop(test[test['date'].apply(findInvalidDates)].index, inplace=True)\n","train.drop(train[train['date'].apply(findInvalidDates)].index, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q98OBVPWeNwu"},"source":["## In order to engineer our features so that we can later feed them to our model we will use pre-trained embeddings."]},{"cell_type":"code","metadata":{"id":"NrX_3dlfeLl5"},"source":["import gensim.downloader as api\n","\n","glove_vectors = api.load(\"glove-wiki-gigaword-100\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J_JpdNkv0_gv"},"source":["##Generating embeddings"]},{"cell_type":"code","metadata":{"id":"S0tCtM6sc8lY"},"source":["def textToEmbedding(txt, embeddings, rmvStopwords=True):\n","    if rmvStopwords:\n","        stopwords = set(STOPWORDS).union({\"say\",\"may\",\"see\",\"will\"})\n","        txt  = [token for token in txt if token not in stopwords]\n","    \n","    ret_vec = np.zeros((1,embeddings.vector_size), dtype=float)\n","    # iterate through words and retrive embedding from pre-trained vectors\n","    counter = 0\n","    for token in txt:\n","        if token not in embeddings.key_to_index.keys():\n","            continue\n","        \n","        ret_vec += embeddings[token]\n","        counter += 1\n","\n","    # embedding for whole text is average of embedding of each word\n","    return (ret_vec / counter) if counter != 0 else ret_vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W38Thn2-c9ol"},"source":["def recordsToVecs(df, embeddings):\n","    for idx, row in enumerate(df.iterrows()):\n","        record = row[1]\n","\n","        title_vec = textToEmbedding(record['title'].split(), embeddings)\n","        text_vec = textToEmbedding(record['text'].split(), embeddings)\n","\n","        record_vec = np.concatenate((title_vec, text_vec)).reshape(1,-1)\n","\n","        if idx == 0:\n","            ret_vec = record_vec\n","        else:\n","            ret_vec = np.concatenate((ret_vec, record_vec))\n","\n","    return ret_vec"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"60jmQ00b9Ldn"},"source":["# shuffle our dataset\n","train = train.sample(frac=1)\n","test = test.sample(frac=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"59aaHqYgnbBQ","collapsed":true},"source":["X_train_w2v = recordsToVecs(train, glove_vectors)\n","X_test_w2v = recordsToVecs(test, glove_vectors)\n","\n","print(f'Train data shape: {X_train_w2v.shape}')\n","print(f'Test data shape: {X_test_w2v.shape}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8J5QaXH2Gbo"},"source":["## Creating the bag of words representations"]},{"cell_type":"code","metadata":{"id":"28VohRyF2Ac7"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=200,\n","stop_words='english', ngram_range=(1,2))\n","\n","X_train_bow = train['title'] + train['text']\n","X_test_bow = test['title'] + test['text']\n","\n","# learn the vocabulary only on training data\n","X_train_bow = bow_vectorizer.fit_transform(X_train_bow.tolist())\n","X_test_bow = bow_vectorizer.transform(X_test_bow.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nM_5KJxv7DlH"},"source":["## Creating the tf-idf representations\n"]},{"cell_type":"code","metadata":{"id":"8K-e-Sgj5naa"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=200,\n","stop_words='english', ngram_range=(1,2))\n","\n","X_train_tfidf = train['title'] + train['text']\n","X_test_tfidf = test['title'] + test['text']\n","\n","# learn the vocabulary only on training data\n","X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_tfidf.tolist())\n","X_test_tfidf = tfidf_vectorizer.transform(X_test_tfidf.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vjHi5H6V1Phr"},"source":["y_train = train['label']\n","y_test = test['label']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NITXzKpU1zVF"},"source":["from sklearn.metrics import f1_score, accuracy_score\n","from sklearn.preprocessing import normalize\n","\n","def wrapper(X_train_w2v,\n","            X_test_w2v,\n","            X_train_tfidf,\n","            X_test_tfidf,\n","            X_train_bow,\n","            X_test_bow,\n","            y_train,\n","            y_test):\n","    def doClassification(model, data_representation='w2v'):\n","        nonlocal X_train_w2v\n","        nonlocal X_test_w2v\n","        nonlocal X_train_tfidf\n","        nonlocal X_test_tfidf\n","        nonlocal X_train_bow\n","        nonlocal X_test_bow\n","        nonlocal y_train\n","        nonlocal y_test\n","\n","        if data_representation == 'w2v':\n","            X_train = X_train_w2v\n","            X_test = X_test_w2v\n","        elif data_representation == 'tfidf':\n","            X_train = X_train_tfidf\n","            X_test = X_test_tfidf\n","        else:\n","            X_train = X_train_bow\n","            X_test = X_test_bow\n","\n","        \n","        # train the given model\n","        model.fit(X_train, y_train)\n","\n","        # predict labels of test set\n","        pred = model.predict(X_test)\n","\n","        # print the results\n","        print(f\"F1 score: {f1_score(y_test, pred).round(2)}\")\n","        print(f\"Accuracy: {(100*accuracy_score(y_test, pred)).round(2)} %\")\n","\n","\n","    return doClassification\n","\n","\n","classify = wrapper(X_train_w2v,\n","                    X_test_w2v,\n","                    X_train_tfidf,\n","                    X_test_tfidf,\n","                    X_train_bow,\n","                    X_test_bow,\n","                    y_train,\n","                    y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sVvtFIgi6uyJ"},"source":["##**Logistic Regression**"]},{"cell_type":"code","metadata":{"id":"KPI9a7Sc5y4-"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","logReg = LogisticRegression(max_iter=300)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVaz7P4D7pBR"},"source":["###Word2vec"]},{"cell_type":"code","metadata":{"id":"NZ0GdFM241qM"},"source":["classify(logReg, 'w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4we3-hD5_lQJ"},"source":["###Tf-idf\n"]},{"cell_type":"code","metadata":{"id":"VtdQ-_mY94p1"},"source":["classify(logReg, 'tfidf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFsG1caI_nxx"},"source":["###Bow\n"]},{"cell_type":"code","metadata":{"id":"Ek7nE2Vw-F9w"},"source":["classify(logReg, 'bow')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VUrqzC46gCs"},"source":["##**Naive Bayes**"]},{"cell_type":"code","metadata":{"id":"S0uvbOms6fWq"},"source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.naive_bayes import GaussianNB\n","\n","gauss = GaussianNB()\n","mult = MultinomialNB()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbjMBhK07LfQ"},"source":["###Word2vec"]},{"cell_type":"code","metadata":{"id":"geMSWe7P7LfR"},"source":["classify(gauss, 'w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xFOaQtTaI3Gx"},"source":["#### The bad result of the naive bayes algorithm in combination with the word embeddings is not a surprise. From theory we have learned that this algorithm makes the assumption that our features are independent, an assumption that is far from true in the vector space we represent our data in."]},{"cell_type":"markdown","metadata":{"id":"WSPAzWQmV7DU"},"source":["###The same is true about the other two representations as we can see below"]},{"cell_type":"markdown","metadata":{"id":"bA_G9AZn7LfS"},"source":["###Tf-idf\n"]},{"cell_type":"code","metadata":{"id":"M_qiWLaq7LfT"},"source":["classify(mult, 'tfidf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q3gkkEUn7LfU"},"source":["###Bow\n"]},{"cell_type":"code","metadata":{"id":"20ecttPy7LfU"},"source":["classify(mult, 'bow')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MtfPvJ2b9iR2"},"source":["##**SVM**"]},{"cell_type":"code","metadata":{"id":"OjYHdiFm9iR-"},"source":["from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV\n","\n","\n","def findBestParams(X, y):\n","    param_grid = [{\n","        'C': [1, 16],\n","        'kernel': ['linear']\n","    }, {\n","    'C': [1, 0.5, 8],\n","    'kernel': ['rbf'], \n","    'gamma': [0.05, 0.1]\n","    }]\n","\n","    svm = SVC()\n","\n","    grid_search = GridSearchCV(svm,\n","                            param_grid,\n","                            cv=3,\n","                            scoring='accuracy',\n","                            return_train_score=True)\n","    \n","    grid_search.fit(X, y)\n","    \n","    print(f'Best parameters: {grid_search.best_params_}')\n","    print(f'Best accuracy: {grid_search.best_score_}')\n","    \n","    return grid_search.best_params_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xOxz-wSx9iR_"},"source":["###Word2vec"]},{"cell_type":"code","metadata":{"id":"7s0uKsZmF0ZE"},"source":["best_params = findBestParams(X_train_w2v, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"urqXea8Y9iR_"},"source":["svm = SVC(C=best_params['C'],\n","          gamma=best_params['gamma'],\n","          kernel=best_params['kernel'])\n","classify(svm, 'w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OARE5W7U9iR_"},"source":["###Tf-idf\n"]},{"cell_type":"code","metadata":{"id":"HHxPfoOgGhdN"},"source":["best_params = findBestParams(X_train_tfidf, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKftdnAc9iR_"},"source":["svm = SVC(C=best_params['C'],\n","          gamma=best_params['gamma'],\n","          kernel=best_params['kernel'])\n","classify(svm, 'tfidf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WG4EMBcZ9iR_"},"source":["###Bow\n"]},{"cell_type":"code","metadata":{"id":"GOr_1RLuGlez"},"source":["best_params = findBestParams(X_train_bow, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k7tVcxvC9iSA"},"source":["svm = SVC(C=best_params['C'],\n","          kernel=best_params['kernel'])\n","classify(svm, 'bow')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"biK1wetHI-fk"},"source":["###Since the best value for kernel hyperparameter chosen by grid search is radial basis function, we can assume that our classes (i.e. true and fake articles) are non-linearly seperable."]},{"cell_type":"markdown","metadata":{"id":"6mKHvVRgHJ-o"},"source":["##**Random Forests**"]},{"cell_type":"code","metadata":{"id":"0xBdC58UHJ_I"},"source":["from sklearn.ensemble import RandomForestClassifier\n","\n","randForest = RandomForestClassifier(n_jobs=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tQ4arqMvHJ_I"},"source":["###Word2vec"]},{"cell_type":"code","metadata":{"id":"3jjwySI-HJ_J"},"source":["classify(randForest, 'w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hu066D-SHJ_J"},"source":["###Tf-idf\n"]},{"cell_type":"code","metadata":{"id":"xR7ar_fpHJ_K"},"source":["classify(randForest, 'tfidf')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cK4iLboUHJ_K"},"source":["###Bow\n"]},{"cell_type":"code","metadata":{"id":"-N9kMO6XHJ_K"},"source":["classify(randForest, 'bow')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j8zO-CVod51a"},"source":["#Beat The Benchmark"]},{"cell_type":"markdown","metadata":{"id":"rBGbibOyd2no"},"source":["## In order to achieve higher scores we are going to engineer new features. First of all, fake news articles tend to contain key words that we have detected in our wordclouds earlier. Secondly, fake news articles use a lot of punctuation to attract attention. Finally we will make use of the 'date' column. The intuition behind using the date is that possibly there is no need for fake articles to be published when there are legit articles to be published. In other words fake and legit articles are not published in the same time periods."]},{"cell_type":"code","metadata":{"id":"xYMRRBFPfYT-"},"source":["def generateNewFeatures(df, fake_news_df, legit_news_df):\n","    # iterate the dataframe's indices\n","    for i, (idx,lbl) in enumerate(zip(df.index.to_list(), df.label.to_list())):\n","        search_df = legit_news_df if lbl == 1 else fake_news_df\n","\n","        # add the number of question marks and exclamation marks to the vector\n","        row = search_df.loc[idx, :].copy()\n","        ttl_punc_count = len(re.sub(r'[^!?]', '', row.title))\n","        txt_punc_count = len(re.sub(r'[^!?]', '', row.text))\n","\n","        # add day month and year to the vector\n","        date = pd.to_datetime(row.date)\n","\n","        hotwords_ttl = row.title.count('video') + row.title.count('tweet') +\\\n","row.title.count('image') + row.title.count('shock') + row.title.count('watch') +\\\n","row.title.count('news') + row.title.count('fake')\n","        hotwords_txt = row.text.count('video') + row.text.count('tweet') +\\\n","row.text.count('image') + row.text.count('shock') + row.title.count('watch') +\\\n","row.title.count('news') + row.title.count('fake')\n","\n","        new_row = np.array([ttl_punc_count,txt_punc_count,\n","                                date.day,\n","                                date.month,\n","                                date.year,\n","                                1 if hotwords_ttl > 0 else 0,\n","                                1 if hotwords_txt > 0 else 0\n","                            ]).reshape(1,-1)\n","\n","        if i == 0:\n","            new_features = new_row\n","        else:\n","            new_features = np.concatenate((new_features, new_row), axis=0)\n","\n","    normalised = (new_features - np.mean(new_features, axis=0)) / np.std(new_features, axis=0)\n","    return normalised"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s0DczNpp_vVi"},"source":["from scipy.sparse import csr_matrix\n","\n","train_new_features = generateNewFeatures(train, fake_news_unproc, legit_news_unproc)\n","test_new_features = generateNewFeatures(test, fake_news_unproc, legit_news_unproc)\n","\n","add_features = (lambda old, new: np.concatenate((old, new), axis = 1))\n","\n","classify_bonus = wrapper(add_features(X_train_w2v, train_new_features),\n","                    add_features(X_test_w2v, test_new_features),\n","                    csr_matrix(add_features(X_train_tfidf.toarray(), train_new_features)),\n","                    csr_matrix(add_features(X_test_tfidf.toarray(), test_new_features)),\n","                    csr_matrix(add_features(X_train_bow.toarray(), train_new_features)),\n","                    csr_matrix(add_features(X_test_bow.toarray(), test_new_features)),\n","                    y_train,\n","                    y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNZxVyOqrMUm"},"source":["## Logistic Regression"]},{"cell_type":"code","metadata":{"id":"RNzOFiNcLYa3"},"source":["classify_bonus(logReg, 'w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFiaRnBkMPG_"},"source":["classify_bonus(logReg, 'tfidf')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxWLSYBNMRps"},"source":["classify_bonus(logReg, 'bow')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8SX33LeGrv5H"},"source":["### The already good accuracy of the logistic regression models shows a slight increase by 1-2%"]},{"cell_type":"markdown","metadata":{"id":"iA1OOSrdrV4F"},"source":["## Naive Bayes\n"]},{"cell_type":"code","metadata":{"id":"J9NNwaTygBKk"},"source":["classify_bonus(gauss, 'w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mEexQItbrcMB"},"source":["### We can see that there is a 4% increase in accuracy of the naive bayes model using word embeddings."]},{"cell_type":"markdown","metadata":{"id":"mzs-ui5XsPBN"},"source":["##Random Forests"]},{"cell_type":"code","metadata":{"id":"rftm-jWrhW55"},"source":["classify_bonus(randForest, 'w2v')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jEyNMxrGsVR4"},"source":["### The model that is trained with word embeddings shows an increase of accuracy by 6%"]},{"cell_type":"code","metadata":{"id":"Mi6PrftwmkuE"},"source":["classify_bonus(randForest, 'tfidf')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n28AVuzimwNj"},"source":["classify_bonus(randForest, 'bow')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jVWd1pGssvxO"},"source":["### Here we can see slight to none increases in score"]}]}