{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"data_visualization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JhQdHvWg9NbR"},"source":["# **Ερωτήματα (Part 1)**\n","---\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pg--2oPkvBo6"},"source":["##*__Loading our datasets__*"]},{"cell_type":"code","metadata":{"id":"14FXmk2VGfqY"},"source":["from google.colab import drive\n","from os import chdir\n","\n","drive.mount(\"/content/drive\")\n","chdir(\"/content/drive/MyDrive/my_project1\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XHmPz--j_2sx"},"source":["#Imports\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import re \n","\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbP9tvoMANb3"},"source":["netfx_df = pd.read_csv(\"netflix_titles.csv\")\n","imdb_mov_df = pd.read_csv(\"imdb-movies.csv\", low_memory=False)\n","imdb_rat_df = pd.read_csv(\"imdb-ratings.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"93ql9Tl2vWKn"},"source":["##*__Data Preprocessing__*"]},{"cell_type":"markdown","metadata":{"id":"-4oY5MVAvn2j"},"source":["###Firstly we will get familiar with our data"]},{"cell_type":"code","metadata":{"id":"D5wb73rRboqO"},"source":["netfx_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lS71Ropm_Nl7"},"source":["imdb_mov_df.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aDwZh6PGb2wY"},"source":["print('~~~~~NETFLIX DATASET~~~~~')\n","print(netfx_df.info())\n","print()\n","print('~~~~~IMDB MOVIES DATASET~~~~~')\n","print(imdb_mov_df.info())\n","print()\n","print('~~~~~IMDB RATINGS DATASET~~~~~')\n","print(imdb_rat_df.info())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O8n66O5rxqma"},"source":["###We can see that some columns, which we will use later on(country, director, cast), have missing values so we will try filling them with the corresponding values from the imdb dataset <br/>"]},{"cell_type":"code","metadata":{"id":"DaXyKtlhyNhS"},"source":["# function that fills null values with the values found in the imdb dataframe\n","def fillNullValues(netflixDf, imdbDf, netflixColumn, imdbColumn):\n","    # first get the rows that have null values in the column\n","    nullRows = netflixDf[netflixDf['type'].isin(['Movie']) &\\\n","                        netflixDf[netflixColumn].isnull()].copy()\n","    # then join the two dataframes by title\n","    joinedRows = imdbDf.set_index('title').join(nullRows.set_index('title'),\n","                                                rsuffix='_net',\n","                                                how='inner')\n","    \n","    # change imdb country abbreviations to full names\n","    joinedRows['country'] = joinedRows['country']\\\n","        .str.replace('USA', 'United States', regex=False)\n","    joinedRows['country'] = joinedRows['country']\\\n","        .str.replace('UK', 'United Kingdom', regex=False)\n","\n","\n","    # make sure a row contains info about the same \n","    # movie from the imdb and netflix dataset\n","    joinedRows = joinedRows[(joinedRows['year'].astype(np.int64) == joinedRows['release_year']) &\\\n","                (joinedRows['country'] == joinedRows['country_net'])]\n","\n","\n","    # add the missing values to the netflix dataframe\n","    for i,row in enumerate(joinedRows.values):\n","        # get the missing value\n","        columnIdx = list(joinedRows.columns).index(imdbColumn)\n","        columnValue = row[columnIdx]\n","        # find the location of the missing value in the netflix dataframe\n","        title = joinedRows.index[i]\n","        rowIdx = netflixDf[netflixDf['title'] == title].index\n","        # fill in the missing value\n","        netflixDf.loc[rowIdx, netflixColumn] = columnValue\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XnpmG5fQ-N5A"},"source":["fillNullValues(netfx_df, imdb_mov_df, 'director', 'director')\n","fillNullValues(netfx_df, imdb_mov_df, 'cast', 'actors')\n","fillNullValues(netfx_df, imdb_mov_df, 'country', 'country')\n","\n","netfx_df.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SN5UQ7XBv5xc"},"source":["###Unfortunately we couldn't make any progress, so we will have to rely on dropping the empty rows because the data we are working with are mostly categorical"]},{"cell_type":"markdown","metadata":{"id":"Sip3QK0CHXtJ"},"source":["##*__Question \\#1__*"]},{"cell_type":"markdown","metadata":{"id":"kL847apAxlVQ"},"source":[">Ποιό είδος υπερτερεί, οι ταινίες ή οι σειρές;"]},{"cell_type":"code","metadata":{"id":"HP9DiqxFHUf-"},"source":["# get type column\n","type_ser = netfx_df['type']\n","# count appearances of each content type on the platform\n","print(f\"Counts per type:\\n{type_ser.value_counts()}\\n\")\n","\n","# plot the result\n","def setAxesContent(ax, title, xlabel, ylabel):\n","    ax.set_title(title)\n","    ax.set_xlabel(xlabel)\n","    ax.set_ylabel(ylabel)\n","\n","sns.set_theme(context='notebook', style='whitegrid',\n","              palette='deep', font_scale=1.5)\n","\n","plt.figure(figsize=(8,6))\n","\n","ax = sns.countplot(x='type',data=netfx_df)\n","setAxesContent(ax, 'Most Popular Content', 'Content type', 'Content amount')\n","\n","plt.show(ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLK8YDXl6LVu"},"source":["##*__Question \\#2__*"]},{"cell_type":"markdown","metadata":{"id":"bmS8p0thHU0V"},"source":[">Τα τελευταία χρόνια το netflix επενδύει περισσότερο σε ταινίες ή σε σειρές;"]},{"cell_type":"code","metadata":{"id":"5kvANceA6b6Z"},"source":["netfx_df.columns"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"57HuKJCZ6ydF"},"source":["###The column we are interested in is *'date_added'*. We will focus on the last 3 years"]},{"cell_type":"code","metadata":{"id":"L3QTvoQV6k9U"},"source":["# only need the content type and the year it was added to the platform\n","df = netfx_df.loc[:, ['type','date_added']].copy()\n","df.dropna(inplace=True)\n","\n","# convert to supported date data type so that we can perform arithmetic\n","date_added_ser = df['date_added'].apply(pd.to_datetime)\n","print(date_added_ser.max())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAnWI148AD-F"},"source":["###From the above output the years we are interested in are 2019, 2020 and 2021"]},{"cell_type":"code","metadata":{"id":"HG80MjVQ-O87"},"source":["# extract the year from every date\n","date_added_ser = date_added_ser.apply(lambda x: x.year)\n","df['year_added'] = date_added_ser\n","\n","# drop if year is older than 2019\n","df = df[df['year_added'] >= 2019]\n","\n","df.groupby('year_added')['type'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XuM82BhgO3fn"},"source":["# plot the result\n","plt.figure(figsize=(12,6))\n","# tidy dataset for plotting\n","df.rename(columns={'type':'Content type'}, inplace=True)\n","\n","ax = sns.countplot(x='year_added', data=df, hue='Content type')\n","setAxesContent(ax, 'Most popular content in last three years',\n","               'Year added to the platform', 'Content amount')\n","\n","plt.show(ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"See4QaiHSEvS"},"source":["###In conclusion, from the graph we can clearly see that netflix invests more into movies"]},{"cell_type":"markdown","metadata":{"id":"TlP1Beg6oDrQ"},"source":["##*__Question \\#3__*"]},{"cell_type":"markdown","metadata":{"id":"DEMjBtZ_oLKv"},"source":[">Ποιά χώρα έχει το περισσότερο περιεχόμενο;"]},{"cell_type":"code","metadata":{"id":"UEQaRbQMoHfw"},"source":["netfx_df.columns.array"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dN8alIxAqpaj"},"source":["###The column we are interested in is *'country'*\n","###Now we need to 'unwrap' the country column since a movie can be produced in multiple countries"]},{"cell_type":"code","metadata":{"id":"XiNHzpJexvi8"},"source":["# function that locates comma separated values in a given column \n","# and splits the corresponding row into multiple rows\n","def unwrapComSepColumn(df, columnName):\n","    def getValues(row):\n","        nonlocal columnName\n","        \n","        # return a list of the comma separated values\n","        if ',' in row[columnName]:\n","            col_str = row[columnName]\n","            col_splitted = col_str.split(',')\n","            return pd.Series(data=[col_splitted], index=[columnName])\n","        \n","        # row is not comma separated, return NaN\n","        return pd.Series(index=[columnName], dtype='object')\n","\n","    valuesUnwrapped = df.apply(getValues, axis=1)\n","    \n","    # for every value found create a new row in initial dataframe\n","    tmp = pd.DataFrame(columns=df.columns.array)\n","    for index, row in valuesUnwrapped.iterrows():\n","        if row.isnull().all():\n","            continue\n","\n","        for val in row[columnName]:\n","            val = val.strip()\n","            if val == \"\":\n","                continue\n","                \n","            newRow = df.loc[index,:].copy()\n","            newRow[columnName] = val\n","            tmp = tmp.append(newRow,ignore_index=True)\n","\n","    df = df.append(tmp,ignore_index=True)\n","    # now drop the rows with the concatenated values\n","    df = df[~df[columnName].str.contains(',', regex=False)]\n","\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYM4nFYjoH8w"},"source":["df = netfx_df[['country']].copy()\n","df.dropna(inplace=True)\n","\n","df = unwrapComSepColumn(df, 'country')\n","\n","# find top ten countries with most content\n","top_ten = df['country'].value_counts().head(10).index.array\n","\n","# only keep those countries\n","df = df[df['country'].isin(top_ten)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4nvzhF1oIHv"},"source":["print(f\"Country with most content:\\n{df['country'].value_counts().head(1)}\\n\")\n","\n","# plot the result\n","plt.figure(figsize=(18,10))\n","plt.xticks(rotation=45)\n","\n","ax = sns.countplot(x='country', data=df, order = top_ten)\n","setAxesContent(ax, 'Top Ten countries with most content',\n","                'Country', 'Content amount')\n","ax.set_yticks(np.linspace(0,3400,num=18))\n","\n","plt.show(ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xXz_RpQP1EIY"},"source":["###United States produce the most content"]},{"cell_type":"markdown","metadata":{"id":"qSF-XiiY12e-"},"source":["##*__Question \\#4__*"]},{"cell_type":"markdown","metadata":{"id":"cSZJAgWw15vd"},"source":[">Τι είδους περιεχόμενο έχει κάθε χώρα;"]},{"cell_type":"markdown","metadata":{"id":"8F1kELJ63VI7"},"source":["###For this question we are going to need columns from netflix and imdb dataframes i.e. the *'country'* and the *'genre'* columns"]},{"cell_type":"code","metadata":{"id":"cmfy405R2Qms"},"source":["df_net = netfx_df[['type','title','country','release_year']].copy()\n","# drop movies that have no country specified\n","df_net.dropna(inplace=True)\n","\n","# only keep the movies since the imdb dataset contains movies\n","df_net = df_net[df_net['type'] == 'Movie']\n","df_net.drop('type', axis=1, inplace=True)\n","\n","df_imdb = imdb_mov_df[['original_title','genre', 'year', 'country']].copy()\n","df_imdb.columns = ['title', 'genre', 'year', 'imdb_country']\n","# drop movies that have no country specified\n","df_imdb.dropna(inplace=True)\n","\n","# change abbreviations to full names\n","df_imdb['imdb_country'] = df_imdb['imdb_country']\\\n","    .str.replace('USA', 'United States', regex=False)\n","df_imdb['imdb_country'] = df_imdb['imdb_country']\\\n","    .str.replace('UK', 'United Kingdom', regex=False)\n","\n","merged = pd.merge(df_net, df_imdb, how='inner', on='title')\n","\n","# keep movies that have the same release year only\n","# to avoid movie remakes and sequels\n","merged['year'] = merged['year'].astype(np.int64)\n","merged = merged[(merged['year'] == merged['release_year']) &\\\n","                (merged['country'] == merged['imdb_country'])]\n","\n","# don't need these columns anymore\n","merged.drop(['year','release_year','imdb_country'], axis=1, inplace=True)\n","\n","# unwrap country column\n","merged = unwrapComSepColumn(merged, 'country')\n","\n","# keep top five countries with most content\n","top_five_countries = merged['country'].value_counts().head().index.to_list()\n","merged = merged[merged['country'].isin(top_five_countries)]\n","\n","merged.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHBsar5i8SXr"},"source":["# find movie counts per genre in each country group\n","by_country = merged.groupby('country')\n","\n","def countGenres(country_gr):\n","    # keep a dictionary per genre along with the number of movies \n","    # of that genre\n","    gnr_dict = {}\n","    # get amount of movies per genre for a specific country group\n","    def getGenres(genre_str):\n","        nonlocal gnr_dict\n","        genres = genre_str.split()\n","        for genre in genres:\n","            genre = genre.replace(\",\",\"\")\n","            if genre not in gnr_dict.keys(): # new genre\n","                gnr_dict[genre] = 1\n","            else: # one more movie of the same genre\n","                gnr_dict[genre] += 1\n","\n","    country_gr['genre'].apply(getGenres)\n","\n","    # unwrap the dictionary\n","    genres, counts = zip(*gnr_dict.items())\n","    # return a row of all counts per genre\n","    return pd.Series(counts, index=genres)\n","\n","df = by_country.apply(countGenres).unstack(level=1)\n","df.fillna(value=0, inplace=True)\n","df = df.astype(np.int64)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RDzLZKIdB64Y"},"source":["# prepare data for plotting\n","df.reset_index(inplace=True)\n","tidy = df.melt(id_vars='country')\n","tidy.sort_values('value', inplace=True, ascending=False)\n","tidy.rename(columns={'variable':'Genre'}, inplace=True)\n","\n","plt.figure(figsize=(28,12))\n","\n","ax = sns.barplot(x='country',y='value', hue='Genre', data=tidy, palette='deep')\n","setAxesContent(ax, 'Popular genres per country', 'Countries', 'Content Amount')\n","ax.set_yticks(np.linspace(0,700,num=21))\n","\n","plt.show(ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kmmtXQk21dhI"},"source":["##*__Question \\#5__*"]},{"cell_type":"markdown","metadata":{"id":"rrtIESig1fwv"},"source":["> Ετοιμάστε γραφήματα που δειχνουν τους ηθοποιούς με τις περισσότερες ταινίες σε κάθε χώρα. Κάντε το ίδιο και για τις σειρές"]},{"cell_type":"markdown","metadata":{"id":"41T5DhqmZOb8"},"source":["###For this question we are going to need the *'type', 'country' and 'cast'* columns from the netflix dataframe"]},{"cell_type":"code","metadata":{"id":"uXAHLg6L1gUv"},"source":["df = netfx_df[['type','country','cast']].copy()\n","# drop the rows with null values\n","df.dropna(inplace=True)\n","\n","# convert concatenated columns to strings \n","df['country'] = df['country'].astype(str)\n","df['cast'] = df['cast'].astype(str)\n","\n","print(df.info())\n","print(df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QyucDSaV1ged"},"source":["df = unwrapComSepColumn(df, 'country')\n","# only keep the top ten countries with most content\n","top_ten_countries = df['country'].value_counts().head(10).index.to_list()\n","df = df[df['country'].isin(top_ten_countries)]\n","df = unwrapComSepColumn(df, 'cast')\n","\n","print(df.info())\n","print(df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvKtdhfHeOWN"},"source":["# split movies and series\n","df_movies = df[df['type'] == 'Movie'].copy()\n","df_series = df[df['type'] == 'TV Show'].copy()\n","# don't need the columns anymore\n","df_movies.drop('type', axis=1, inplace=True)\n","df_series.drop('type', axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYMYNPvUfbqD"},"source":["def getTop10ActorsPerCntr(df):\n","    # for each country get number of actor appearances\n","    counts_ser = df.groupby('country')['cast'].value_counts()\n","\n","    tmp_dict = {}\n","    # from every country keep top 10 actors with most appearances\n","    for country in counts_ser.index.get_level_values(0).unique():\n","        # find the top 10 actors of specific country\n","        top_10 = counts_ser[country].head(10).values\n","        # only keep rows that belong to current country\n","        bools = list(map(lambda x: x == country, counts_ser.index.get_level_values(0)))\n","        \n","        tmp_dict.update(counts_ser[counts_ser.isin(top_10) & bools].head(10).to_dict())\n","\n","    top_10_act = pd.Series(tmp_dict)\n","\n","    return top_10_act.unstack(level = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxOqAqvHvznl"},"source":["# calculate top actor appearances\n","df_movies = getTop10ActorsPerCntr(df_movies)\n","df_series = getTop10ActorsPerCntr(df_series)\n","\n","df_movies.reset_index(inplace=True)\n","df_series.reset_index(inplace=True)\n","\n","df_movies.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PwlRH9RLurDi"},"source":["\n","def plotTopActors(df):\n","\n","    # prepare dataset for plotting\n","    tidy = df.melt(id_vars='index')\n","    tidy.dropna(inplace=True)\n","    tidy.sort_values('value', inplace=True, ascending=False)\n","    tidy.rename(columns={'variable':'Actor', 'index':'Country'}, inplace=True)\n","\n","    countries = tidy['Country'].unique()\n","    num_countries = tidy['Country'].nunique()\n","\n","    fig, _ = plt.subplots(squeeze=False, figsize=(24,30))\n","    fig.tight_layout()\n","    sns.set_theme(context='notebook', style='whitegrid',\n","                palette='deep', font_scale=1.0)\n","    \n","    for row in range(5):\n","        for col in range(2):\n","            # get a country\n","            country_idx = row*2 + col\n","            if country_idx > num_countries-1:\n","                break\n","            ax = plt.subplot2grid((5, 2), (row, col))\n","            country = countries[country_idx]\n","            tmp = tidy[tidy['Country'] == country]\n","            sns.barplot(y='Actor',x='value', ax=ax, data=tmp, palette='deep')\n","            setAxesContent(ax, f'Popular actors in {country}', 'Appearances', '')\n","            \n","    plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LuzgoWZkBLXp"},"source":["### Plotting for the Movies"]},{"cell_type":"code","metadata":{"id":"shtRu4bZ9BMQ"},"source":["plotTopActors(df_movies)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9dCIFtBIBQew"},"source":["###Plotting for the TV Shows"]},{"cell_type":"code","metadata":{"id":"8SqtxhR-4IqN"},"source":["plotTopActors(df_series)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OmHKEzZJB1DU"},"source":["##*__Question \\#6__*"]},{"cell_type":"markdown","metadata":{"id":"lZ0_9XwSB3eX"},"source":[">Το netflix υποστηρίζει ότι παρέχει πλούσιο περιεχόμενο για όλες τις ηλικίες.Αληθεύει αυτό; Φτιάξτε ένα γράφημα το οποίο συγκεντρώνει το πλήθος των ταινιων ανάλογα με την προτεινόμενη ηλικία (αναφερόμαστε στη στήλη rating). Τα όρια καθορίζονται σύμφωνα με τον παρακάτω πίνακα. Κάντε το ίδιο γράφημα και για τις σειρές."]},{"cell_type":"markdown","metadata":{"id":"9ifblD64K689"},"source":["###We will only need the *'type' and 'rating'* columns for this question"]},{"cell_type":"code","metadata":{"id":"5bnCKDWxGiJC"},"source":["df = netfx_df[['type', 'rating']].copy()\n","df.dropna(inplace = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F9pB923lK3oN"},"source":["# function that turns values from rating column into age categories\n","def getRatingAges(df):\n","    bins = {'little_kids': ['G', 'TV-Y', 'TV-G'],\n","            'older_kids': ['PG', 'TV-Y7', 'TV-Y7-FV', 'TV-PG'],\n","            'teens': ['PG-13', 'TV-14']\n","            }\n","    def getBin(rating):\n","        nonlocal bins\n","        if rating in bins['little_kids']:\n","            return 'Little Kids'\n","        elif rating in bins['older_kids']:\n","            return 'Older Kids'\n","        elif rating in bins['teens']:\n","            return 'Teens'\n","        else:\n","            return 'Mature'\n","\n","    df['ages'] = df['rating'].apply(getBin)\n","\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mw-OSd1wK4Lc"},"source":["df = getRatingAges(df)\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AuqQ-5tL7vV"},"source":["def plotAges(group):\n","    if group.name == 'Movie':\n","        ax = plt.subplot2grid((1, 2), (0, 0))\n","    else:\n","        ax = plt.subplot2grid((1, 2), (0, 1))\n","\n","    sns.countplot(x='ages', data=group, ax=ax)\n","\n","    if group.name == 'Movie':\n","        setAxesContent(ax, 'Movies per Age Category','Age Groups', 'Content amount')\n","    else:\n","        setAxesContent(ax, 'TV Shows per Age Category', 'Age Groups', 'Content amount')\n","\n","fig, _ = plt.subplots(squeeze=False, figsize=(14,6))\n","fig.tight_layout()\n","sns.set_theme(context='notebook', style='whitegrid',\n","                palette='deep', font_scale=1.5)\n","\n","df.groupby('type').apply(plotAges)  \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LgM97sB7PSgb"},"source":["##*__Question \\#7__*"]},{"cell_type":"markdown","metadata":{"id":"qCJOJb0hPYqL"},"source":[">Αν ένας παραγωγός ήθελε να έχει υψηλή ακροαματικότητα, σκέφτεται ότι θα ήταν ίσως καλύτερα να βγάλει την ταινία του σε μία εποχή που δεν υπάρχει ανταγωνισμός. Κάντε ένα γράφημα με το περιεχόμενο που προστίθεται ανά μήνα για να τον βοηθήσετε να επιλέξει τη σωστή χρονική περίοδο."]},{"cell_type":"code","metadata":{"id":"jbP9q1roQy4t"},"source":["\n","\n","df = netfx_df[['date_added','type']].copy()\n","df.dropna(inplace=True)\n","# only keep the movies\n","df = df[df['type'] == 'Movie']\n","\n","plt.figure(figsize=(12,6))\n","plt.xticks(rotation= 45)\n","\n","df['month'] = df['date_added'].apply(lambda x: re.sub(\"[^A-Za-z]*\",\"\",x))\n","ax = sns.countplot(x='month', data=df, palette ='deep',\\\n","                   order= df['month'].value_counts().index.to_list())\n","setAxesContent(ax, 'Content added per month', 'Month', 'Content amount')\n","ax.set_yticks(np.linspace(0,600,num=11))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8UWgMLiOW_To"},"source":["###From the above we can see that the producer should publish his movie on February"]},{"cell_type":"markdown","metadata":{"id":"yTR7wXv3XbP6"},"source":["##*__Question \\#8__*"]},{"cell_type":"markdown","metadata":{"id":"yLP3b9t7XbnY"},"source":[">Ετοιμάστε ένα γράφημα που παρουσιάζει συγκεντρωτικά τα είδη του περιεχομένου (αναφερόμαστε στη στήλη listed_in)."]},{"cell_type":"code","metadata":{"id":"qXg6bknjXsTK"},"source":["df = netfx_df[['listed_in']].copy()\n","\n","# the listed_in column has a lot of concatenated values\n","# unwrap it\n","df = unwrapComSepColumn(df, 'listed_in')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j8xWTY9waBI3"},"source":["# plot the results\n","plt.figure(figsize=(18,14))\n","\n","sns.set_theme(context='notebook', style='whitegrid', font_scale=1.5)\n","\n","ax = sns.countplot(y='listed_in',data=df,\\\n","                   order= df['listed_in'].value_counts().index, palette='deep')\n","setAxesContent(ax, 'Content per category Netflix', 'Content amount', 'Category')\n","ax.set_xticks(np.linspace(0,2500,num=21))\n","\n","plt.show(ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQlbMHnOimZu"},"source":["##*__Question \\#9__*"]},{"cell_type":"markdown","metadata":{"id":"aO9bleATilZ8"},"source":[">Μελετηστε τους σκηνοθέτες ανά χώρα και παρουσιάστε σχετικά γραφήματα."]},{"cell_type":"code","metadata":{"id":"zRAvAwtLdL53"},"source":["# we will need the director and the country column\n","df = netfx_df[['director', 'country']].copy()\n","df.dropna(inplace=True)\n","\n","df = unwrapComSepColumn(df, 'country')\n","df = unwrapComSepColumn(df, 'director')\n","\n","print(df.info())\n","print(df.head())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYCIagspjTkk"},"source":["def plotDirectors(df):\n","    row, col = 0, 0\n","    def plotMethod(group):\n","        nonlocal row, col\n","        ax = plt.subplot2grid((5, 2), (row, col))\n","\n","        # find ten directors with most content\n","        ten_popular = group['director'].value_counts().head(10).index.to_list()\n","        country = group.name\n","        group = group[group['director'].isin(ten_popular)]\n","        \n","        # plot the result\n","        sns.countplot(y='director', data=group, ax=ax,\\\n","                      order = group['director'].value_counts().index)\n","        \n","        setAxesContent(ax, f'Directors with most movies in {country}','','')\n","        ax.set_xticks(np.linspace(0,15,num=16))\n","\n","        # update axes indices for next group\n","        if col == 1:\n","            row += 1\n","            col = 0\n","        else:\n","            col += 1\n","\n","\n","    fig, _ = plt.subplots(squeeze=True, figsize=(24,30))\n","    fig.tight_layout()\n","    sns.set_theme(context='notebook', style='whitegrid',\n","                    palette='deep', font_scale=1.0)\n","\n","    df.groupby('country').apply(plotMethod)  \n","    plt.show()\n","\n","# plot top ten countries\n","top_ten_countries = df['country'].value_counts().head(10).index.to_list()\n","df = df[df['country'].isin(top_ten_countries)]\n","plotDirectors(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e6DaMmjHnXNt"},"source":["##*__Question \\#10__*"]},{"cell_type":"markdown","metadata":{"id":"2FLhQKy-twTs"},"source":[">Μελετήστε τις σειρές και παρουσιάστε ένα γράφημα που τις δείχνει ανάλογα με το αριθμό των seasons."]},{"cell_type":"code","metadata":{"id":"VWZP_kKNx7O6"},"source":["df = netfx_df[['title', 'type', 'duration', 'date_added', 'country']].copy()\n","df.dropna(inplace=True)\n","# only keep the TV series\n","df = df[df['type'] == 'TV Show']\n","\n","# get amount of seasons in new column\n","df['seasons'] = df['duration'].apply(lambda x: (np.int64)(re.sub(\"[^0-9]*\",\"\",x)))\n","\n","df['date_added'] = pd.to_datetime(df['date_added'])\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7kieWBjwYlh"},"source":["###Plot all the TV Shows w.r.t their seasons and the year they were added to the platform, in an interactive plot"]},{"cell_type":"code","metadata":{"id":"nnwKU_WcyOyj"},"source":["import altair as alt\n","\n","df.rename(columns={'title':'Show title', 'country':'Country',\n","                   'seasons':'Total Seasons','date_added':'Date added'},inplace=True)\n","\n","alt.Chart(df, width=1000, height=700).mark_circle(size=50).encode(\n","    x='Date added',\n","    y='Total Seasons',\n","    color='Country',\n","    tooltip=['Show title', 'Country', 'Total Seasons', 'Date added']\n",").interactive()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2PffIld0Moi"},"source":["##*__Question \\#11__*"]},{"cell_type":"markdown","metadata":{"id":"5X6zaLuO0ONK"},"source":[">Αξιοποιήστε τα υπόλοιπα αρχεία της εργασίας για να απαντήσετε στο ερώτημα:\n","Ποιές είναι οι ταινίες με την πιο υψηλή βαθμολογία; Θα χρειαστεί να συννενώσετε\n","κατάλληλα τα αρχεία που σας δίνονται ώστε να κρατήσετε τις ταινίες του netflix για τις οποίες υπάρχουν ratings στο IMBD"]},{"cell_type":"code","metadata":{"id":"STQcahU-nzPY"},"source":["#We want only 'imdb_title_id' and 'title' from the imdb movies data frame\n","dfm = imdb_mov_df[['imdb_title_id','title','year','country']].copy()\n","#We want only 'imdb_title_id' and 'weighted_average_vote' from the imdb ratings data frame\n","dfr = imdb_rat_df[['imdb_title_id','weighted_average_vote',\\\n","                   'allgenders_30age_avg_vote','allgenders_18age_avg_vote',\\\n","                   'allgenders_45age_avg_vote','females_allages_avg_vote',\\\n","                   'males_allages_avg_vote']].copy()\n","\n","#Join the two new data frames on the column that they have in common ('imdb_title_id')\n","df = dfm.join(dfr.set_index('imdb_title_id'), on='imdb_title_id', how='inner')\n","\n","#We want to connect this data frame to netflix data frame so we keep only 'type' and 'title'\n","dfn = netfx_df[['type','title', 'release_year', 'country']].copy()\n","#Keep only the movies\n","dfn = dfn[dfn['type'] == 'Movie']\n","\n","#Join on 'title' to a final data frame\n","df = df.join(dfn.set_index('title'), on='title', how='inner', rsuffix='_net')\n","\n","# Make sure the columns from netflix and imdb datasets refer to the same movie\n","# change imdb country abbreviations to full names\n","df['country'] = df['country']\\\n","    .str.replace('USA', 'United States', regex=False)\n","df['country'] = df['country']\\\n","    .str.replace('UK', 'United Kingdom', regex=False)\n","\n","df = df[(df['year'].astype(np.int64) == df['release_year'].astype(np.int64)) &\\\n","        (df['country'] == df['country_net'])]\n","\n","df.sort_values('weighted_average_vote', ascending=False, inplace=True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6N9UUKviqBoE"},"source":["sns.set_theme(context='notebook', style='whitegrid', font_scale=1.5)\n","plt.figure(figsize=(15,15))\n","\n","ax = sns.barplot(y='title',x='weighted_average_vote', data=df.head(25), palette='deep')\n","ax.set_xticks(np.linspace(0,10,num=21))\n","setAxesContent(ax, 'Top IMDb-Rating Movies on Netflix', 'Average Rating', 'Netflix Movie')\n","plt.show(ax)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rLjn61RkhUWj"},"source":["###Now we will show ratings in different ages and genders"]},{"cell_type":"code","metadata":{"id":"EgEhCgSEhdGP"},"source":["# only keep the columns we are going to need\n","df.drop(['type','imdb_title_id','year','release_year','weighted_average_vote',\\\n","         'country', 'country_net'], axis=1, inplace=True)\n","df.rename(columns={'allgenders_18age_avg_vote':'Age 18',\n","                   'allgenders_30age_avg_vote':'Age 30',\n","                   'allgenders_45age_avg_vote':'Age 45',\n","                   'females_allages_avg_vote':'Females',\n","                   'males_allages_avg_vote':'Males'}, inplace=True)\n","df = df.melt(id_vars='title')\n","\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Can0xifDk1F0"},"source":["def plotRatings(df):\n","    row, col = 0, 0\n","    def plotMethod(group):\n","        nonlocal row, col\n","        ax = plt.subplot2grid((3, 2), (row, col))\n","        ttl = group.name\n","        \n","        # sort in order to find top 10 most rated movies\n","        group = group.sort_values('value', ascending=False)\n","        \n","        # plot the result\n","        sns.barplot(y='title', x='value', palette='deep', data=group.head(10), ax=ax)\n","        setAxesContent(ax, f'Top rated movies in {ttl}','Rating','')\n","        ax.set_xticks(np.linspace(0,10,num=21))\n","\n","        # update axes indices for next group\n","        if col == 1:\n","            row += 1\n","            col = 0\n","        else:\n","            col += 1\n","\n","\n","    fig, _ = plt.subplots(squeeze=True, figsize=(24,15))\n","    fig.tight_layout()\n","    sns.set_theme(context='notebook', style='whitegrid', font_scale=1.0)\n","\n","    df.groupby('variable').apply(plotMethod)  \n","    plt.show()\n","\n","# plot ratings per age and gender\n","plotRatings(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2y_4JgAVrL7l"},"source":["# **Part 2 - Recommendation system**\n","---"]},{"cell_type":"markdown","metadata":{"id":"NVwXZdF_GfNc"},"source":["## *__Data Preprocessing__*"]},{"cell_type":"code","metadata":{"id":"qzAfZ7P8SfCm"},"source":["import nltk\n","\n","# Keep only the columns we need\n","df = netfx_df[['show_id', 'title', 'description']].copy()\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"swqsUmMtUclD"},"source":["# Importing the WordNetLemmatizer module from nltk.stem\n","nltk.download('wordnet')\n","\n","from nltk.corpus.reader.wordnet import NOUN\n","from nltk.corpus import wordnet\n","\n","class WordNetLemmatizer(object):\n","  def __init__(self):\n","        pass\n","\n","  def lemmatize(self, word, pos=NOUN):\n","        lemmas = wordnet._morphy(word, pos)\n","        return min(lemmas, key=len) if lemmas else word\n","\n","\n","  def __repr__(self):\n","        return \"<WordNetLemmatizer>\"\n","\n","\n","\n","# unload wordnet\n","def teardown_module(module=None):\n","    from nltk.corpus import wordnet\n","\n","    wordnet._unload()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slLWPjKZGuGx"},"source":["###The preprocessing steps we will apply are:\n","1. Convert all letters to lowercase\n","2. Remove punctuation\n","3. Apply lemmatization"]},{"cell_type":"code","metadata":{"id":"MviJdGG9Uvnl"},"source":["# Create WordNetLemmatizer object\n","lemmatizer = WordNetLemmatizer()\n","\n","# Text pre-processing function using regular expresions and lemmatizer\n","def preProc(desc):\n","    # Make all letters lowercase\n","    desc = desc.lower()\n","    # Cleaning the description from special characters i.e. punctuation\n","    desc = re.sub(r'[^\\w\\- ]','',desc)\n","\n","    cleanDesc = []\n","    for word in desc.split():\n","        word = lemmatizer.lemmatize(word)\n","        cleanDesc.append(word)\n","\n","    return \" \".join(cleanDesc)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p400ePqhYVhn"},"source":["# Do the cleaning\n","df['clean_description'] = df['description'].apply(preProc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qqqex2faZtA3"},"source":["df.head(10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oq4-VBPBTbqQ"},"source":["## **1.**"]},{"cell_type":"markdown","metadata":{"id":"-n08U6NGeIeg"},"source":["###*a)*"]},{"cell_type":"code","metadata":{"id":"gV9k9iLGZyAR"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","desc_list = df['clean_description'].tolist()\n","\n","# Create the boolean vectorizer using CountVectorizer\n","bow_vectorizer = CountVectorizer(max_df=1.0, min_df=1, max_features=1000,\n","stop_words='english', ngram_range=(1,2))\n","\n","# Boolean representation of unigrams and bigrams of movies descriptions\n","BoWs = bow_vectorizer.fit_transform(desc_list).toarray()\n","\n","print(BoWs.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"INvrrdDBTtBG"},"source":["###*b)*"]},{"cell_type":"code","metadata":{"id":"3LmYLLFhbeIe"},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Create the TF-IDF vectorizer using CountVectorizer\n","tfidf_vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, max_features=1000,\n","stop_words='english', ngram_range=(1,2))\n","\n","# TF-IDF representation of unigrams and bigrams of movies descriptions\n","tf_idf = tfidf_vectorizer.fit_transform(desc_list).toarray()\n","\n","print(tf_idf.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"luIPKBNmTwCv"},"source":["## **2.**"]},{"cell_type":"markdown","metadata":{"id":"Z9ZwwZ2jIGnG"},"source":["### Below we define our metric functions"]},{"cell_type":"code","metadata":{"id":"V9NFSX2teNDe"},"source":["\"\"\"\n","Similarity finders\n","\"\"\"\n","# Define our own jaccard similarity method\n","def jaccard_coeffient(a,b):\n","    return np.minimum(a,b).sum()/np.maximum(a,b).sum()\n","\n","# Define our own cosine similarity method\n","def cosine_similarity(a,b):\n","    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uYg3tAQbIQVu"},"source":["###For every movie we will keep two maxheaps that contain the most similar movies as to the jaccard coefficient metric and the cosine similarity metric"]},{"cell_type":"code","metadata":{"id":"E7WK3g-_EgKJ"},"source":["# Function to update each movie's heap\n","def tryPushToHeap(id_to_push, jac_heap, cos_heap, jac_sim, cos_sim, N_most=100):\n","    # check jaccard heap\n","    if len(jac_heap) < N_most: # heap not full, just push the new movie\n","        heappush(jac_heap, (jac_sim, id_to_push))\n","    elif nsmallest(1, jac_heap)[0][0] < jac_sim: # new movie is more similar than the least similar\n","        _ = heapreplace(jac_heap, (jac_sim, id_to_push))\n","\n","    # now check cosine heap \n","    if len(cos_heap) < N_most:\n","        heappush(cos_heap, (cos_sim, id_to_push))\n","    elif nsmallest(1, cos_heap)[0][0] < cos_sim: # If heap is full, check if the smallest value is less than the new cosine simularity\n","        _ = heapreplace(cos_heap, (cos_sim, id_to_push))\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PLJmIofEIsLQ"},"source":["###The algorithm we implement below iterates through every combination of two movies and calculates the pair's metric scores. The scores will be stored to the movies' heaps if they are better than the existing scores or if the heaps are not yet filled with the N most similar movies."]},{"cell_type":"code","metadata":{"id":"gM17ipVjr8UU"},"source":["from heapq import heappush, heapify, nlargest, nsmallest, heapreplace\n","\"\"\" \n","This will be a dictionary that contains show_id's as keys \n","and two maxheaps, one for each metric (jaccard, cosine), with 100 most similar movies along with their scores\n","\"\"\"\n","top_100_sim = {}\n","# Initialize the heaps\n","for id in df['show_id']:\n","    top_100_sim[id] = {'jaccard':[], 'cosine':[]}\n","    heapify(top_100_sim[id]['jaccard'])\n","    heapify(top_100_sim[id]['cosine'])\n","\n","\n","for i in range(df.index.size):\n","    mov_id = df.iloc[i, df.columns.to_list().index('show_id')] # Find the movie id\n","\n","    # Find the movie's heaps\n","    jac_heap = top_100_sim[mov_id]['jaccard']\n","    cos_heap = top_100_sim[mov_id]['cosine']\n","\n","    for z in range(i + 1, df.index.size):\n","        new_mov_id = df.iloc[z, df.columns.tolist().index('show_id')]\n","\n","        jac_sim = jaccard_coeffient(BoWs[i], BoWs[z]) # Calculate the similarity using jaccard\n","        cos_sim = cosine_similarity(tf_idf[i], tf_idf[z]) # Calculate the similarity using cosine\n","\n","        # Find the new movie's heaps\n","        new_mov_jac_heap = top_100_sim[new_mov_id]['jaccard']\n","        new_mov_cos_heap = top_100_sim[new_mov_id]['cosine']\n","\n","        # Updating the two movie heaps simultaneously\n","        tryPushToHeap(new_mov_id, jac_heap, cos_heap, jac_sim, cos_sim)\n","        tryPushToHeap(mov_id, new_mov_jac_heap, new_mov_cos_heap, jac_sim, cos_sim)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u9GGq83RJlal"},"source":["## **3.**"]},{"cell_type":"markdown","metadata":{"id":"Dq71M59YJ8MK"},"source":["###Now we will just access the dictionary we created from the previous step using the key (*'show_id'* column in dataset) that corresponds to the movie's title and we will retrieve the N movies with the highest scores from the two max heaps."]},{"cell_type":"code","metadata":{"id":"A-uQ6ba9HH-W"},"source":["def get_similar_movies1(title, N=10, method='boolean'):\n","\n","    # Using the title, find the movie ID\n","    movie_id = df[df['title'] == title]['show_id'].values[0]\n","    if method == 'boolean':\n","        top = nlargest(N, top_100_sim[movie_id]['jaccard']) # Keep the N most similar movies using Jaccard as metric\n","    elif method == 'tf-idf':\n","        top = nlargest(N, top_100_sim[movie_id]['cosine']) # Keep the N most similar movies using Cosine as metric\n","    \n","    # Print the results\n","    print(f\"Queried movie's description:\\n\\\n","{df[df['title'] == title]['description'].values[0]}\\n\\nRecommendations:\")\n","\n","    for i, (score, movie_id) in enumerate(top):\n","            row = df[df['show_id'] == movie_id]\n","            print(f'~~~~~~{i+1}~~~~~~')\n","            print(f\"Title: {row['title'].values[0]} | Similarity: {(score*100).round(2)}%\")\n","            print(f\"Description: {row['description'].values[0]}\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqnMyFqNkcCd"},"source":["get_similar_movies1('Inception', 10, 'tf-idf')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWVdh1opITAw"},"source":["get_similar_movies1('Inception', 10, 'boolean')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BufffhJeJoT_"},"source":["##4."]},{"cell_type":"markdown","metadata":{"id":"LeQVIimWOvYk"},"source":["###From the movie description we are given we will create a new vector representation by using our pretrained vectorizer and then we will just find the N most similar movies based on the description."]},{"cell_type":"code","metadata":{"id":"f1_ZwU-lJpme"},"source":["# Wrapper function so that the 'get_similar_movies2' can use the vectorizer and the two different representations\n","def wrapper(bow_vectorizer, tf_vectorizer, df, BoWs, tf_idf):\n","    def get_similar_movies2(desc, N=10, method='tf-idf'):\n","        nonlocal bow_vectorizer, tf_vectorizer, df, BoWs, tf_idf\n","        \n","        if method == 'boolean':\n","            vectorizer = bow_vectorizer\n","            vecs = BoWs\n","        else:\n","            vectorizer = tf_vectorizer\n","            vecs = tf_idf\n","\n","        desc_clean = preProc(desc) # Clean the new description\n","        movie_vec = vectorizer.transform([desc_clean]).toarray() # Vectorize the cleaned description\n","\n","        heap = []\n","        heapify(heap)\n","\n","        # Calculate the similarity of that vector with all the other vectors in the representation (BoW or tf-idf)\n","        for i in range(df.index.size):\n","\n","            if method == 'boolean':\n","                score = jaccard_coeffient(movie_vec[0], vecs[i])\n","            else:\n","                score = cosine_similarity(movie_vec[0], vecs[i])\n","            \n","            mov_id = df.iloc[i, df.columns.to_list().index('show_id')]\n","            # Fill the heap of similarities that corresponds to the description given\n","            heappush(heap, (score, mov_id))\n","\n","        # Keep the N most similar\n","        top = nlargest(N, heap)\n","\n","        # Printing the results\n","        for i, (score, movie_id) in enumerate(top):\n","            row = df[df['show_id'] == movie_id]\n","            print(f'~~~~~~{i+1}~~~~~~')\n","            print(f\"Title: {row['title'].values[0]} | Similarity: {(score*100).round(2)}%\")\n","            print(f\"Description: {row['description'].values[0]}\\n\")\n","\n","            \n","    return get_similar_movies2\n","\n","get_similar_movies2 = wrapper(bow_vectorizer, tfidf_vectorizer, df, BoWs, tf_idf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3RKh2wOtu24K"},"source":["get_similar_movies2(\"War between America and Vietnam\", N=10, method='tf-idf')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bpc3MugGnmhX"},"source":["get_similar_movies2(\"War between America and Vietnam\", N=10, method='boolean')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z0Us5r8pcYFl"},"source":["# ***Παρατηρήσεις***\n","Όπως ήταν αναμενόμενο, η μέθοδος TF-IDF μας δίνει καλύτερα αποτελέσματα με υψηλότερα ποσοστά ομοιότητας.\n","\n","Συγκεκριμένα, όσων αφορά την ‘get_similar_movies1’ η πρώτη πιο όμοια ταινία του Inception με την μέθοδο boolean μας επιστρέφει ένα ποσοστό ομοιότητας της τάξης του 20% ενώ με την μέθοδο TF-IDF, μέχρι και η δεκάτη πιο όμοια ταινία έχει ένα ποσοστό μεγαλύτερο από αυτό (25.25%).  \n","\n","Όσων αφορά την ‘get_similar_movies2’, πάλι βλέπουμε ακόμα μεγαλύτερη διάφορα στις ομοιότητες. Με την μέθοδο tf-idf η most similar ταινία που αντιστοιχεί στην δοθείσα περιγραφή, διαθέτει ποσοστό ομοιότητας 46.39% ενώ με την μέθοδο boolean μόλις 25%.\n","\n","Είναι απόλυτα λογικό το γεγονός ότι οι δυο υλοποιήσεις μας επιστρέφουν διαφορετικά αποτελέσματα, διότι έχουν και άλλον τρόπο κατά το vectorizing άλλα και άλλο τρόπο σύγκρισης των vector. \n","\n","Με τα δεδομένα που διαθέτουμε, αναμέναμε καλύτερα αποτελέσματα με την tf-df μέθοδο όπως προαναφέραμε άλλα σε συνδυασμό με την cosine similarity, είχαμε ακόμη καλύτερα αποτελέσματα. Αυτό συμβαίνει διότι όταν έχουμε επανάληψη δεδομένων, εχει αποδειχθεί πως η cosine είναι καλύτερος τρόπος σύγκρισης από την jaccard.\n","\n","Επίσης, παρατηρείται πως μεταξύ των N πιο ομοίων ταινιών, η διαφοροποίηση στην ομοιότητα που έχουμε, κάνοντας χρήση την μέθοδο boolean, είναι πολύ μικρή. Και για τα δυο ερωτήματα (3,4) η απόκλιση της πιο όμοιας ταινίας με την N πιο όμοια είναι κατά μέσο όρο ~4%. Αυτό σημάνει πως η μέθοδο δεν είναι πολύ λεπτομερείς και δεν μπορεί να εντοπίσει εύκολα διάφορες ή ομοιότητες.\n","Στα ίδια ερωτήματα, με την μέθοδο tf-idf η ίδια απόκλιση είχε τιμή ~ 9%, που όσο το N ανεβαίνει τόσο θα ανεβαίνει.\n","\n"]}]}